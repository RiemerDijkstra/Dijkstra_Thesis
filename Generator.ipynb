{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3984b84a",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27bd9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install transformers\n",
    "# !python3 -m pip install statsmodels\n",
    "# !python3 -m pip install tensorflow==1.4.1\n",
    "# !python3 -m pip install torch\n",
    "# !python3 -m pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d41bb45",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mle import Mandelbrot\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# from gensim.corpora.wikicorpus import extract_pages, filter_wiki, process_article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4512232",
   "metadata": {},
   "source": [
    "# Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d742015",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final, cleaned wikipedia file\n",
    "subset = open(\"subset.txt\", \"r\").read()\n",
    "\n",
    "texts = subset.split('</doc>')[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880c74d",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da04bb",
   "metadata": {},
   "source": [
    "### Prompt Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(article, begin=True):\n",
    "  \n",
    "    article_sents = nltk.tokenize.sent_tokenize(article)\n",
    "    sent_num = len(article_sents)\n",
    "\n",
    "    if begin is True:\n",
    "      if sent_num >= 3:\n",
    "        return ' '.join(article_sents[:3])\n",
    "      if sent_num >= 2:\n",
    "        return ' '.join(article_sents[:2])\n",
    "      else:\n",
    "        return article_sents[0]\n",
    "    elif sent_num >= 3:\n",
    "      return ' '.join(article_sents[-4:])\n",
    "    elif sent_num >= 2:\n",
    "      return ' '.join(article_sents[-2:])\n",
    "    else:\n",
    "      return article_sents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0b3dc2",
   "metadata": {},
   "source": [
    "### Section Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860776ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_section(section, min_tokens, k, p, t, rep_pen, n_gram):\n",
    "    \n",
    "    prompt = make_prompt(section, begin=True)\n",
    "    prompt_len = len(prompt.split())\n",
    "        \n",
    "    in_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    in_ids = in_ids.to(\"cuda\")\n",
    "    \n",
    "    curr_id_len = len(in_ids[0])\n",
    "    max_len = min_len = curr_id_len+512\n",
    "\n",
    "    out = []\n",
    "    \n",
    "    while len(out) <= min_tokens:\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "            \n",
    "        out_ids = model.generate(\n",
    "            in_ids,\n",
    "            do_sample=True,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            top_k=k,\n",
    "            top_p=p,\n",
    "            temperature=t,\n",
    "            repetition_penalty=rep_pen,\n",
    "            no_repeat_ngram_size=n_gram\n",
    "        )\n",
    "        \n",
    "        output = tokenizer.decode(out_ids[0], skip_special_tokens=True).split()      \n",
    "        output_no_prompt = output[prompt_len:]\n",
    "        out.extend(output_no_prompt)\n",
    "        \n",
    "        output_full = \" \".join(output)\n",
    "        prompt = make_prompt(output_full, begin=False)\n",
    "        prompt_len = len(prompt.split())\n",
    "                \n",
    "        in_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        in_ids = in_ids.to(\"cuda\")\n",
    "        \n",
    "        # Error control\n",
    "        if len(in_ids[0]) == curr_id_len+128:\n",
    "            \n",
    "            out_sents = nltk.tokenize.sent_tokenize(\" \".join(out))\n",
    "            if len(out_sents) > 1:\n",
    "                new_sents = \" \".join(out_sents[1:])\n",
    "            else:\n",
    "                new_sents = \" \".join(out_sents)\n",
    "    \n",
    "            prompt = make_prompt(new_sents, begin=True)\n",
    "            prompt_len = len(prompt.split())\n",
    "            \n",
    "            in_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "            in_ids = in_ids.to(\"cuda\")\n",
    "              \n",
    "        \n",
    "        curr_id_len = len(in_ids[0])\n",
    "        max_len = min_len = curr_id_len+512\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c36525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(txt, k, p, t, rep_pen, n_gram):\n",
    "    \n",
    "    new_tokens = []\n",
    "\n",
    "    sections = txt.split('\\n\\n')\n",
    "    for section in sections:\n",
    "        sec_len = len(section.split())\n",
    "        if sec_len > 10:\n",
    "            \n",
    "            curr_tokens = generate_section(\n",
    "                section, \n",
    "                min_tokens=sec_len, \n",
    "                k=k, \n",
    "                p=p, \n",
    "                t=t, \n",
    "                rep_pen=rep_pen, \n",
    "                n_gram=n_gram\n",
    "            )\n",
    "            \n",
    "            new_tokens.extend(curr_tokens[:sec_len] + [\"\\n\\n\"])\n",
    "\n",
    "    new_txt = \" \".join(new_tokens)\n",
    "    return new_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2672e",
   "metadata": {},
   "source": [
    "### Generation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6de6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for txt in texts:\n",
    "    print(i)\n",
    "    curr_txt = generate(txt, k=None, p=0.95, t=1, rep_pen=1.0, n_gram=3)\n",
    "    with open(\"gen_set.txt\", \"a\", encoding='utf-8') as handle:\n",
    "        handle.write(curr_txt)\n",
    "        handle.write(\"\\n\\n</doc>\\n\\n\\n\\n\")\n",
    "    print(\": completed\")\n",
    "#     print(curr_txt)\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
