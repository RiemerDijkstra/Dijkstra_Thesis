{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The domestic dog (Canis familiaris or Canis lupus familiaris)[4] is a domesticated wolf. The dog descended from an ancient, extinct wolf,[5][6] with the modern grey wolf being the nearest living relative.[7] The dog was the first species to be domesticated,[8][7] by hunter–gatherers over 15,000 years ago,[6] before the development of agriculture.'\n",
    "\n",
    "f = open(\"gpt2.txt\", \"a\")\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "tf.random.set_seed(5)\n",
    "    \n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    min_length=1000,\n",
    "    max_length=1500,\n",
    "    top_p=0.95,\n",
    "    top_k=40\n",
    ")\n",
    "    \n",
    "output = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "\n",
    "f.write(output)\n",
    "f.write('\\nEND\\n')\n",
    "    \n",
    "f.close()    \n",
    "\n",
    "\n",
    "# 3min 42s\n",
    "# 10.000 articles = 616.67 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distilGPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"distil_gpt2.txt\", \"a\")\n",
    "\n",
    "prompt = 'The domestic dog (Canis familiaris or Canis lupus familiaris)[4] is a domesticated wolf. The dog descended from an ancient, extinct wolf,[5][6] with the modern grey wolf being the nearest living relative.[7] The dog was the first species to be domesticated,[8][7] by hunter–gatherers over 15,000 years ago,[6] before the development of agriculture.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "tf.random.set_seed(5)\n",
    "    \n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    min_length=200,\n",
    "    max_length=500,\n",
    "    top_p=0.95,\n",
    "    top_k=40\n",
    ")\n",
    "    \n",
    "output = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "    \n",
    "f.write(output)\n",
    "f.write('\\nEND\\n')\n",
    "    \n",
    "f.close()   \n",
    "\n",
    "\n",
    "# 2min 56s\n",
    "# 10.000 articles = 489 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prompt = 'hi, this is a test.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "tf.random.set_seed(0)\n",
    "    \n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    min_length=200,\n",
    "    max_length=300,\n",
    "    top_k=40\n",
    ")\n",
    "    \n",
    "output = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(output)\n",
    "\n",
    "# Unusable output\n",
    "# 2min 40s for one sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BBC news & distilGPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = dataset[\"train\"].to_dict()\n",
    "test_dict = dataset[\"test\"].to_dict()\n",
    "val_dict = dataset[\"validation\"].to_dict()\n",
    "\n",
    "docs = train_dict[\"document\"] + test_dict[\"document\"] + val_dict[\"document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "f = open(\"dataset.txt\", \"a\")\n",
    "\n",
    "for item in docs[336:]:\n",
    "  try:\n",
    "    # generate prompt\n",
    "    tokens = nlp(item)\n",
    "    i = 0\n",
    "    prompt = ''\n",
    "    for sent in tokens.sents:\n",
    "        i += 1\n",
    "        prompt = prompt + ' ' + sent.string.strip()\n",
    "        # 3 sentences\n",
    "        if i > 3:\n",
    "          break\n",
    "\n",
    "    prompt_size = len(prompt)\n",
    "\n",
    "    # encode input\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    tf.random.set_seed(3)\n",
    "    \n",
    "    # generate output\n",
    "    sample_output = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        min_length=prompt_size+200,\n",
    "        max_length=prompt_size+400,\n",
    "        # top_p & top_k sampling\n",
    "        top_p=0.95,\n",
    "        top_k=50\n",
    "\n",
    "        )\n",
    "  \n",
    "    # decode output\n",
    "    output = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # write output, excluding the prompt\n",
    "    f.write(output[len(prompt):])\n",
    "\n",
    "  # handle max token error (max=1024)\n",
    "  except IndexError:\n",
    "    pass\n",
    "\n",
    "f.close()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
