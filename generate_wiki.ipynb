{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The domestic dog (Canis familiaris or Canis lupus familiaris)[4] is a domesticated wolf. The dog descended from an ancient, extinct wolf,[5][6] with the modern grey wolf being the nearest living relative.[7] The dog was the first species to be domesticated,[8][7] by hunter–gatherers over 15,000 years ago,[6] before the development of agriculture.'\n",
    "\n",
    "f = open(\"gpt2.txt\", \"a\")\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "tf.random.set_seed(5)\n",
    "    \n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    min_length=1000,\n",
    "    max_length=1500,\n",
    "    top_p=0.95,\n",
    "    top_k=40\n",
    ")\n",
    "    \n",
    "output = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "\n",
    "f.write(output)\n",
    "f.write('\\nEND\\n')\n",
    "    \n",
    "f.close()    \n",
    "\n",
    "\n",
    "# 3min 42s\n",
    "# 10.000 articles = 616.67 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distilGPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"distil_gpt2.txt\", \"a\")\n",
    "\n",
    "prompt = 'The domestic dog (Canis familiaris or Canis lupus familiaris)[4] is a domesticated wolf. The dog descended from an ancient, extinct wolf,[5][6] with the modern grey wolf being the nearest living relative.[7] The dog was the first species to be domesticated,[8][7] by hunter–gatherers over 15,000 years ago,[6] before the development of agriculture.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "tf.random.set_seed(5)\n",
    "    \n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    min_length=200,\n",
    "    max_length=500,\n",
    "    top_p=0.95,\n",
    "    top_k=40\n",
    ")\n",
    "    \n",
    "output = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "    \n",
    "f.write(output)\n",
    "f.write('\\nEND\\n')\n",
    "    \n",
    "f.close()   \n",
    "\n",
    "\n",
    "# 2min 56s\n",
    "# 10.000 articles = 489 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prompt = 'hi, this is a test.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "tf.random.set_seed(0)\n",
    "    \n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    min_length=200,\n",
    "    max_length=300,\n",
    "    top_k=40\n",
    ")\n",
    "    \n",
    "output = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(output)\n",
    "\n",
    "# Unusable output\n",
    "# 2min 40s for one sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BBC news & distilGPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = dataset[\"train\"].to_dict()\n",
    "test_dict = dataset[\"test\"].to_dict()\n",
    "val_dict = dataset[\"validation\"].to_dict()\n",
    "\n",
    "docs = train_dict[\"document\"] + test_dict[\"document\"] + val_dict[\"document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "f = open(\"dataset.txt\", \"a\")\n",
    "\n",
    "for item in docs[336:]:\n",
    "  try:\n",
    "    # generate prompt\n",
    "    tokens = nlp(item)\n",
    "    i = 0\n",
    "    prompt = ''\n",
    "    for sent in tokens.sents:\n",
    "        i += 1\n",
    "        prompt = prompt + ' ' + sent.string.strip()\n",
    "        # 3 sentences\n",
    "        if i > 3:\n",
    "          break\n",
    "\n",
    "    prompt_size = len(prompt)\n",
    "\n",
    "    # encode input\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    tf.random.set_seed(3)\n",
    "    \n",
    "    # generate output\n",
    "    sample_output = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        min_length=prompt_size+200,\n",
    "        max_length=prompt_size+400,\n",
    "        # top_p & top_k sampling\n",
    "        top_p=0.95,\n",
    "        top_k=50\n",
    "\n",
    "        )\n",
    "  \n",
    "    # decode output\n",
    "    output = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # write output, excluding the prompt\n",
    "    f.write(output[len(prompt):])\n",
    "\n",
    "  # handle max token error (max=1024)\n",
    "  except IndexError:\n",
    "    pass\n",
    "\n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Wiki dump & distilGPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riemer/.local/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from gensim.corpora.wikicorpus import extract_pages, filter_wiki, process_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Wikipedia dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_raw(raw):\n",
    "    txt = filter_wiki(raw).strip()\n",
    "    txt = re.sub(\"\\n+\", \"\\n\", txt)\n",
    "    return re.sub(\"'+\", \"'\", txt)\n",
    "\n",
    "def dump_to_list(filename, max_articles=1e10):\n",
    "    with open(filename) as handle:\n",
    "        pg_iter = extract_pages(handle)\n",
    "        for (title, raw_text, page_id), _ in tqdm(zip(pg_iter, range(max_articles))):\n",
    "            yield title, text_from_raw(raw_text), int(page_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:11, 837.08it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_file = list(dump_to_list(\"./enwiki-latest-pages-articles16.xml-p20460153p20570392\", max_articles=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = list()\n",
    "\n",
    "# obtain a list of article contents (500 tokens)\n",
    "for article in wiki_file:\n",
    "    if 300 < len(article[1].split()) < 700:\n",
    "        contents.append(article[1])\n",
    "\n",
    "\n",
    "len(contents)        \n",
    "\n",
    "\n",
    "# Do not mind this\n",
    "\n",
    "# j = 0\n",
    "\n",
    "# for item in contents:\n",
    "#     prompt = ''\n",
    "#     for i in range(3):\n",
    "#         prompt += ' ' + nltk.tokenize.sent_tokenize(item)[i]\n",
    "#     print(prompt)\n",
    "#     j += 1 \n",
    "#     if j == 3:\n",
    "#         break\n",
    "    \n",
    "\n",
    "# temp = nltk.tokenize.sent_tokenize(contents[1])\n",
    "\n",
    "# new = ''\n",
    "# for i in range(3):\n",
    "#     new += ' '+temp[i]\n",
    "    \n",
    "# print(new)\n",
    "\n",
    "# wiki_gen.txt bevat 651 samples nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('wiki_human.txt', 'w')\n",
    "# for item in contents[:653]:\n",
    "#     f.write(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riemer/.local/lib/python3.7/site-packages/transformers/generation_utils.py:965: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n",
      "  UserWarning,\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1266 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "f = open(\"wiki_gen.txt\", \"a\")\n",
    "\n",
    "for item in contents[280:]:\n",
    "  try:\n",
    "    # generate prompt\n",
    "    prompt = ''\n",
    "    for i in range(3):\n",
    "        prompt += ' ' + nltk.tokenize.sent_tokenize(item)[i]\n",
    "\n",
    "    # this actually needs to be split()\n",
    "    prompt_size = len(prompt)\n",
    "\n",
    "    # encode input\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    tf.random.set_seed(10)\n",
    "    \n",
    "    # generate output\n",
    "    sample_output = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        min_length=prompt_size+200,\n",
    "        max_length=prompt_size+400,\n",
    "        # top_p & top_k sampling\n",
    "        top_p=0.94\n",
    "#         top_k=50\n",
    "\n",
    "        )\n",
    "  \n",
    "    # decode output\n",
    "    output = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # write output, excluding the prompt\n",
    "    f.write(output[prompt_size+1:])\n",
    "    f.write('\\n\\n\\n\\n')\n",
    "    \n",
    "    counter += 1 \n",
    "    \n",
    "    if counter == 350:\n",
    "        break\n",
    "    \n",
    "  # handle max token error (max=1024)\n",
    "  except IndexError:\n",
    "    pass\n",
    "\n",
    "f.close()   \n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The 'Assistant Secretary of State for Consular Affairs' is the head of the Bureau of Consular Affairs within the United States Department of State.  The Assistant Secretary of State for Consular Affairs reports to the Under Secretary of State for Management.  From 1953 to 1977, the position was called 'Administrator of the Bureau of Security and Consular Affairs'.\\n==List of the Assistant Secretaries of State for Security and Consular Affairs, 1953—77==\\nName\\nAssumed office\\nLeft office\\nPresident(s) served under\\nR. W. Scott McLeod\\nMarch 3, 1953\\nMarch 9, 1957\\n Dwight D. Eisenhower\\nRoderic L. O'Connor\\nMay 28, 1957\\nDecember 29, 1958\\nJohn W. Hanes III\\nJanuary 1, 1959\\nOctober 4, 1962\\nDwight D. Eisenhower and John F. Kennedy\\nAbba P. Schwartz\\nOctober 5, 1962\\nMarch 6, 1966\\nJohn F. Kennedy and Lyndon B. Johnson\\nBarbara M. Watson\\nAugust 12, 1968\\nDecember 31, 1974\\nLyndon B. Johnson and Richard Nixon\\nLeonard F. Walentynowicz\\nJanuary 2, 1975\\nMarch 7, 1977\\nGerald Ford\\n==List of the Assistant Secretaries of State for Consular Affairs, 1977–present==\\nName\\nAssumed office\\nLeft office\\nPresident(s) served under\\nBarbara M. Watson\\nApril 13, 1977\\nAugust 17, 1980\\nJimmy Carter\\nDiego C. Asencio\\nAugust 29, 1980\\nNovember 21, 1983\\nJimmy Carter and Ronald Reagan\\nJoan M. Clark\\nDecember 22, 1983\\nOctober 18, 1989\\nRonald Reagan\\nElizabeth M. Tamposi\\nOctober 18, 1989\\nNovember 10, 1992\\nGeorge H. W. Bush\\nMary A. Ryan\\nMay 12, 1993\\nSeptember 30, 2002 \\nBill Clinton and George W. Bush\\nMaura Harty\\nNovember 20, 2002\\nFebruary 29, 2008\\nGeorge W. Bush\\nJanice L. Jacobs\\nJune 10, 2008\\nApril 4, 2014\\nGeorge W. Bush and Barack Obama\\nMichele T. Bond\\nApril 3, 2014\\nJanuary 26, 2017\\nBarack Obama and Donald Trump\\nCarl Risch\\nAugust 11, 2017\\nDecember 22, 2020\\nDonald Trump\\n==References==\\n==External links==\\n* Information about the Assistant Secretary of State for Consular Affairs by the State Department Historian\\n* Bureau of Consular Affairs Website\", '\\'Hilde Mangold\\' (20 October 1898 – 4 September 1924) (née Proescholdt) was a German embryologist who was best known for her 1923 dissertation which was the foundation for her mentor, Hans Spemann\\'s, 1935 Nobel Prize in Physiology or Medicine for the discovery of the embryonic organizer, \"one of the very few doctoral theses in biology that have directly resulted in the awarding of a Nobel Prize\". The general effect she demonstrated is known as embryonic induction, that is, the capacity of some cells to direct the developmental trajectory of other cells. Induction remains a fundamental concept and area of ongoing research in the field.\\n==Biography==\\nHilde Proescholdt was born in Gotha, Thuringia, a province in central-eastern Germany on October 20, 1898. She was the middle daughter of soap factory owner Ernest Proescholdt and his wife Gertrude. She attended the University of Jena in Germany for two semesters in 1918 and 1919 and then transferred to the University of Frankfurt in Germany where she also spent two semesters. It was here that she saw a lecture by the renowned embryologist Hans Spemann on experimental embryology.\\nThis lecture inspired her to pursue her education in this field. After Frankfurt, she attended the Zoological Institute in Freiburg. It was here that she met and married her husband, , who was Spemann’s chief assistant (and, incidentally, a supporter of the Nazi Party). Under Spemann\\'s direction, she completed her 1923 dissertation, entitled “Über Induktion von Embryonalanlagen durch Implantation artfremder Organisatoren”, or “Induction of Embryonic Primordia by Implantation of Organizers from a Different Species.”\\n \\nAfter earning her PhD in zoology, Hilde moved with her husband and infant son, Christian, to Berlin. Shortly after the move, Hilde died from severe burns as a result of a gas heater explosion in her Berlin home. She never lived to see the publication of her thesis results. Her son died in World War II.\\n==Key experiments==\\nMangold performed very delicate transplantation experiments with embryos (a feat even more impressive before the discovery of antibiotics to prevent infection after surgery). She demonstrated that tissue from the dorsal lip of the blastopore grafted into a host embryo can induce the formation of an extra body axis, creating conjoined twins. Crucially, by using two  species of newt with different skin colors for host and donor, she showed that the amphibian organizer did not form the extra axis by itself, but recruited host tissue to form the twin (although the full implications of this result were not understood until a year after her death). This was the basis of the discovery of the \"organizer\", which is responsible for gastrulation.\\n==See also==\\n* Developmental biology\\n* Embryogenesis\\n* Theodor Boveri\\n* August Weismann\\n==References==\\n==External links==\\n*  Explanation of the Spemann-Mangold experiment from a Nature Reviews article']\n"
     ]
    }
   ],
   "source": [
    "f = open('wiki_human.txt', 'w')\n",
    "\n",
    "for article in contents:\n",
    "    f.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
