{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2073c6f",
   "metadata": {},
   "source": [
    "# WikiExtractor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f4398",
   "metadata": {},
   "source": [
    "Made use of the WikiExtractor module. However, the module is capable of cleaning wiki dump files very thoroughly. Section headers are extracted and cleaned in a way that make section segmentation difficult (which is important later on). The script extractor.py extracts section titles such as '==History==' and replaces them by the word that is contained within the title. So in this particular case, the title would be transformed into 'History.' (a full stop is added if no punctuation is present already). The deleted equal signs seem to be good candidates for split points in order to facilitate section segmentation. Therefore, I have chosen to preserve section titles, which means that WikiExtractor keeps titles in their original states. To achieve this, a small alteration in the extractor.py script had to be made. Originally, the script makes use of regex matching to obtain section titles. Then, different parts of the title are assigned to different variables by using the ***group()*** function. Let's say that variable ***m*** stands for the match between a regex (for section titles) and a section title (==History==, for instance). Then m.group(2) would include 'History' and m.group(1) would include '=='. m.group(2) would later be used to add to the newly cleaned file (and therefore discarding the equal signs). The small alteration that was made, includes changing m.group(2) to m.group(0), since m.group(0) matches with '==History==' for this particular case. On top of this, the lines that are concerned with adding punctuation to the title were commented. They were no longer needed, as the last character of each section title should just be an equal sign.\n",
    "\n",
    "See screenshots for differences between original code and altered version of that code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59054aa",
   "metadata": {},
   "source": [
    "I have tried a different approach by using WikiExtractor. This is what I did:\n",
    "\n",
    "1. Terminal: python3 -m wikiextractor.WikiExtractor enwiki-latest-pages-articles16.xml-p20460153p20570392.bz2 \n",
    "\n",
    "2. Convert the preprocessed data into 1 txt file: cat text/\\*/* > jawiki.txt (https://memotut.com/how-to-use-wikiextractor.py-72753/)\n",
    "\n",
    "The result can be seen in wiki.txt. I got rid of the doc id's (<doc id= ...............>) and wrote the output in a new file: new_wiki.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dumps = ['enwiki-latest-pages-articles1.xml-p1p41242.bz2',\n",
    "              'enwiki-latest-pages-articles3.xml-p151574p311329.bz2',\n",
    "              'enwiki-latest-pages-articles14.xml-p13159683p14324602.bz2',\n",
    "              'enwiki-latest-pages-articles23.xml-p44788942p46288941.bz2',\n",
    "              'enwiki-latest-pages-articles20.xml-p34308443p35522432.bz2',\n",
    "              'enwiki-latest-pages-articles19.xml-p30121851p31308442.bz2',\n",
    "              'enwiki-latest-pages-articles25.xml-p61525656p62585850.bz2',\n",
    "              'enwiki-latest-pages-articles12.xml-p8554860p9172788.bz2',\n",
    "              'enwiki-latest-pages-articles5.xml-p558392p958045.bz2',\n",
    "              'enwiki-latest-pages-articles16.xml-p17460153p18960152.bz2'\n",
    "             ]\n",
    "\n",
    "\n",
    "for dump in wiki_dumps:\n",
    "    !python3 -m wikiextractor.WikiExtractor dump\n",
    "    !cat text/*/* > temp.txt\n",
    "\n",
    "    \n",
    "# i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m wikiextractor.WikiExtractor enwiki-latest-pages-articles16.xml-p20460153p20570392.bz2\n",
    "!cat text/*/* > jawiki.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_in = open('jawiki.txt', 'r')\n",
    "file_out = open('Batch/batch_%i.txt' % i, 'w')\n",
    "\n",
    "skip = False\n",
    "for line in file_in:\n",
    "    if line[:8] == '<doc id=':\n",
    "        skip = True\n",
    "        pass\n",
    "    elif skip == True:\n",
    "        skip = False\n",
    "        pass\n",
    "    else:\n",
    "        file_out.write(line)\n",
    "        \n",
    "file_in.close()\n",
    "file_out.close()\n",
    "i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cf7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Batch/* > mother.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b67173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_perc(txt):\n",
    "    count = 0\n",
    "    for char in txt:\n",
    "        if char.isalpha():\n",
    "            count += 1\n",
    "    if len(txt) != 0:\n",
    "        return (count/len(txt)) * 100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "full_dump = open(\"Batch/batch_10.txt\", \"r\").read()\n",
    "texts = full_dump.split('</doc>')\n",
    "\n",
    "i = 0\n",
    "for txt in texts:\n",
    "    txt = txt.split('=')\n",
    "    temp = []\n",
    "    for section in txt:\n",
    "        if len(section.split()) > 64:\n",
    "            temp.append(section)\n",
    "    txt = ''.join(temp)\n",
    "    if alpha_perc(txt) > 75:\n",
    "        texts[i] = txt\n",
    "    else:\n",
    "        texts[i] = ''\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "f = open('eggie.txt', 'w')\n",
    "l = []\n",
    "\n",
    "for txt in texts:\n",
    "    if 200 <= len(txt.split()) <= 2500:\n",
    "        l.append(txt)\n",
    "        \n",
    "# random.shuffle(l)\n",
    "shuff_part = l[:1000]\n",
    "for txt in shuff_part:\n",
    "    f.write(txt)\n",
    "    f.write('\\n\\n<\\doc>\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dump = open(\"subset.txt\", \"r\").read()\n",
    "\n",
    "texts = full_dump.split('<\\doc>')\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def alpha_perc(txt):\n",
    "    count = 0\n",
    "    for char in txt:\n",
    "        if char.isalpha():\n",
    "            count += 1\n",
    "    if len(txt) != 0:\n",
    "        return (count/len(txt)) * 100\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0d471a",
   "metadata": {},
   "source": [
    "# Loop (extracting subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66016f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('subset.txt', 'a')\n",
    "\n",
    "for i in range(10):\n",
    "    j = 0\n",
    "    i += 1 \n",
    "#     full_dump = open(\"Batch/batch_%i.txt\" %i, \"r\").read()\n",
    "    with open(\"Batch/batch_%i.txt\" %i) as g:\n",
    "        full_dump = g.readlines()\n",
    "        full_dump = ''.join(full_dump)\n",
    "\n",
    "    texts = full_dump.split('</doc>')\n",
    "\n",
    "    for txt in texts:\n",
    "        txt = txt.split('=')\n",
    "        temp = []\n",
    "        for section in txt:\n",
    "            if len(section.split()) > 64:\n",
    "                temp.append(section)\n",
    "        txt = ''.join(temp)\n",
    "        if alpha_perc(txt) > 75:\n",
    "            texts[j] = txt\n",
    "        else:\n",
    "            texts[j] = ''\n",
    "        j += 1\n",
    "    \n",
    "    l = []\n",
    "\n",
    "    for txt in texts:\n",
    "        if 200 <= len(txt.split()) <= 2500:\n",
    "            l.append(txt)\n",
    "        \n",
    "    random.seed(i)\n",
    "    print(i)\n",
    "    random.shuffle(l)\n",
    "    shuff_part = l[:1000]\n",
    "#     shuffle = random.sample(l, 1000)\n",
    "    for txt in shuff_part:\n",
    "        f.write(txt)\n",
    "        f.write('\\n\\n</doc>\\n\\n')\n",
    "    \n",
    "    g.close()\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4205feb",
   "metadata": {},
   "source": [
    "# Loop (cleaning batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    j = 0\n",
    "    i += 1 \n",
    "#     full_dump = open(\"Batch/batch_%i.txt\" %i, \"r\").read()\n",
    "    with open(\"Batch/batch_%i.txt\" %i) as f:\n",
    "        full_dump = f.readlines()\n",
    "        full_dump = ''.join(full_dump)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    texts = full_dump.split('</doc>')\n",
    "\n",
    "    for txt in texts:\n",
    "        txt = txt.split('=')\n",
    "        temp = []\n",
    "        for section in txt:\n",
    "            if len(section.split()) > 64:\n",
    "                temp.append(section)\n",
    "        txt = ''.join(temp)\n",
    "        if alpha_perc(txt) > 75:\n",
    "            texts[j] = txt\n",
    "        else:\n",
    "            texts[j] = ''\n",
    "        j += 1\n",
    "    \n",
    "    g = open('Batch_new/betch_%i.txt' % i, 'w')\n",
    "\n",
    "    random.seed(i)\n",
    "    random.shuffle(texts)\n",
    "    for txt in texts:\n",
    "        if 200 <= len(txt.split()) <= 2500:\n",
    "            g.write(txt)\n",
    "            g.write('\\n\\n</doc>\\n\\n')\n",
    "\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Batch_new/* > mother.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd549e82",
   "metadata": {},
   "source": [
    "# Extra cleaning (Removing incomplete articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ea604",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Batch_new/betch_1.txt\") as f:\n",
    "    dump_1 = f.readlines()\n",
    "    dump_1 = ''.join(dump_1)\n",
    "\n",
    "f.close()\n",
    "\n",
    "with open(\"Batch_new/betch_2.txt\") as f:\n",
    "    dump_2 = f.readlines()\n",
    "    dump_2 = ''.join(dump_2)\n",
    "\n",
    "f.close()\n",
    "\n",
    "texts_1 = dump_1.split('</doc>')\n",
    "texts_2 = dump_2.split('</doc>')\n",
    "\n",
    "concat = texts_1+texts_2\n",
    "\n",
    "no_duplicates = list(set(texts_1) - set(texts_2))\n",
    "\n",
    "duplicates = list(set())\n",
    "\n",
    "texts_1 = list(set(texts_1) - set(duplicates))\n",
    "texts_2 -= dupli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    j = 0\n",
    "    i += 1 \n",
    "#     full_dump = open(\"Batch/batch_%i.txt\" %i, \"r\").read()\n",
    "    with open(\"Batch_new/betch_%i.txt\" %i) as f:\n",
    "        full_dump = f.readlines()\n",
    "        full_dump = ''.join(full_dump)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    texts = full_dump.split('</doc>')\n",
    "    print(len(texts))\n",
    "    \n",
    "    new_set = []\n",
    "    \n",
    "    for txt in texts:\n",
    "        keep = True\n",
    "        tokens = txt.split()\n",
    "        for token in tokens:\n",
    "          if token == '.' or token == ',':\n",
    "            keep = False\n",
    "            break\n",
    "        if keep:\n",
    "            new_set.extend([txt])\n",
    "    \n",
    "    print(len(new_set))\n",
    "        \n",
    "    g = open('Batch/batch_%i.txt' % i, 'w')\n",
    "    for txt in new_set:\n",
    "        g.write(txt)\n",
    "        g.write('\\n\\n</doc>\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7dcd75",
   "metadata": {},
   "source": [
    "# Extract 1.000 samples per randomized batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = []\n",
    "\n",
    "for i in range(10):\n",
    "    i += 1 \n",
    "#     full_dump = open(\"Batch/batch_%i.txt\" %i, \"r\").read()\n",
    "    with open(\"Batch/batch_%i.txt\" %i) as f:\n",
    "        full_dump = f.readlines()\n",
    "        full_dump = ''.join(full_dump)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    texts = full_dump.split('</doc>')\n",
    "    texts = texts[:1000]\n",
    "\n",
    "# initially\n",
    "#     for txt in texts:\n",
    "#         f.write(txt)\n",
    "#         f.write('\\n\\n</doc>\\n\\n')\n",
    "\n",
    "    for txt in texts:\n",
    "        subset.extend([txt])\n",
    "\n",
    "print(len(subset))\n",
    "subset = set(subset)\n",
    "print(len(subset))\n",
    "\n",
    "g = open('subset.txt', 'w')\n",
    "for txt in subset:\n",
    "    g.write(txt)\n",
    "    g.write('</doc>')\n",
    "    \n",
    "g.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
